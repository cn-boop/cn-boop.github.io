<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>BERT系列之详解ALBERT</title>
    <url>/2020/04/25/BERT%E7%B3%BB%E5%88%97%E4%B9%8B%E8%AF%A6%E8%A7%A3ALBERT/</url>
    <content><![CDATA[<p>自BERT出现之后，NLP领域取得了很大的进展，并且随着加大模型的容量，BERT模型的进度也在各个数据集上都有一些提升，但虽然提升模型的大小是能对下游任务的效果有一定的提升，但是如果进一步提升模型规模，势必会导致显存或者内存出现OOM1的问题，长时间的训练也可能是导致模型出现退化的情况。那么如何在大幅度减少模型参数的情况下，却几乎不影响bert模型精度呢？于是便有了————ALBERT</p>
<h2 id="一、回顾bert的参数来源"><a href="#一、回顾bert的参数来源" class="headerlink" title="一、回顾bert的参数来源"></a>一、回顾bert的参数来源</h2><p><img src="/Users/chennuo/Desktop/%E6%88%AA%E5%B1%8F2020-04-25%E4%B8%8A%E5%8D%8810.46.58.png" alt="image.png"><br>bert是基于transformer实现的，我在此处截取了bert中transformer中的encoder端；transformer中的参数来源主要来源于下方（红色方框）的 token embedding projection block 和上方（蓝色方框）的attention-ffn block</p>
<h4 id="Attention-feed-forward-block-80-："><a href="#Attention-feed-forward-block-80-：" class="headerlink" title="Attention feed-forward block(80%)："></a>Attention feed-forward block(80%)：</h4><ul>
<li>Parameter size  O(L x H x H)  </li>
<li>L: layer size</li>
<li>hidden size – needs to be large</li>
</ul>
<h4 id="Token-embedding-projection-block-20"><a href="#Token-embedding-projection-block-20" class="headerlink" title="Token embedding projection block(20%):"></a>Token embedding projection block(20%):</h4><ul>
<li><p>Parameter size O(V x E)</p>
</li>
<li><p>V: vocab size</p>
</li>
<li><p>E: embedding size –often the same size as H</p>
</li>
</ul>
<h2 id="二、The-method-of-ALBert"><a href="#二、The-method-of-ALBert" class="headerlink" title="二、The method of ALBert"></a>二、The method of ALBert</h2><p>既然已经清楚了bert的主要参数来源，那么就来看一下这一篇论文里面减少模型参数具体的措施。</p>
<h4 id="Method1-factorized-embedding-parametrization（矩阵因式分解）"><a href="#Method1-factorized-embedding-parametrization（矩阵因式分解）" class="headerlink" title="Method1:factorized embedding parametrization（矩阵因式分解）"></a>Method1:factorized embedding parametrization（矩阵因式分解）</h4><p><em>前提分析</em></p>
<ul>
<li>Token embeddings are context independent</li>
</ul>
<p>while hidden layer embeddings are context dependent.</p>
<ul>
<li>Token embeddings are sparsely updated.<br><img src="/Users/chennuo/Desktop/%E6%88%AA%E5%B1%8F2020-04-25%E4%B8%8A%E5%8D%8810.47.06.png" alt="image.png"><br>这个idea的来源主要是基于两点：一个就是one-hot vector进行更新时，只会更新每个单词的one-hot vector对应的那个1相关的参数（也就是图中黑色的圆圈）；<br>另外一点就是在进行one-hot embedding projection的过程中是没有单词之间的交互，只有到了后面的attention部分才有单词之间的interaction，因此基于以上两点，便有了下面的做法：<br><img src="/Users/chennuo/Desktop/%E6%88%AA%E5%B1%8F2020-04-25%E4%B8%8A%E5%8D%8810.47.13.png" alt="image.png"><br>先讲单词的one-hot vector（V-dimension） 投影到一个低纬度（E），才从这个低纬度投影投影到模型所需要的纬度(H)</li>
<li><strong>结果</strong>：O(VxH)➡️O(VxE+ExH),其中E&lt;&lt;H</li>
<li><strong>实验结果</strong>：<br><img src="/Users/chennuo/Desktop/%E6%88%AA%E5%B1%8F2020-04-25%E4%B8%8A%E5%8D%8810.47.18.png" alt="image.png"><br>可以从图中看到，E从768变为128以后，模型参数减少了17%，但是模型的精度只下降了0.6%<h4 id="Method2-Cross-layer-parameter-sharing（跨层参数共享）"><a href="#Method2-Cross-layer-parameter-sharing（跨层参数共享）" class="headerlink" title="Method2:Cross-layer parameter sharing（跨层参数共享）"></a>Method2:Cross-layer parameter sharing（跨层参数共享）</h4>第二个方法很简单，也就是<strong>同时</strong>共享transformer模块里面的attention和ffn的参数</li>
<li><strong>结果</strong>：O(LxHxH)➡️O(HxH)</li>
<li><strong>实验结果</strong>：<br><img src="/var/folders/t6/plnyvhg57ss3py62ht9nb_180000gn/T/TemporaryItems/%EF%BC%88screencaptureui%E6%AD%A3%E5%9C%A8%E5%AD%98%E5%82%A8%E6%96%87%E7%A8%BF%EF%BC%8C%E5%B7%B2%E5%AE%8C%E6%88%9018%EF%BC%89/%E6%88%AA%E5%B1%8F2020-04-25%E4%B8%8A%E5%8D%8810.52.04.png" alt="image.png"><br>可以看到，参数共享以后，模型的参数量减少了大概百分之七十，但是模型精度减少了不到百分之三。</li>
</ul>
<h4 id="Combing-method-1-and-method-2"><a href="#Combing-method-1-and-method-2" class="headerlink" title="Combing method 1 and method 2"></a>Combing method 1 and method 2</h4><p>论文里还展示了，两个方法同时结合起来的实验效果：<br><img src="/Users/chennuo/Desktop/%E6%88%AA%E5%B1%8F2020-04-25%E4%B8%8A%E5%8D%8810.47.41.png" alt="image.png"></p>
<p>可以看到，结合两个方法以后，ALbert -xxlarge版本模型的宽度使Bert-large的四倍，但是参数量却只有其百分之七十，而且在五个数据集上的实验结果却高了3.5%。</p>
<h4 id="Design-better-self-supervised-learning-tasks"><a href="#Design-better-self-supervised-learning-tasks" class="headerlink" title="Design better self-supervised learning tasks"></a>Design better self-supervised learning tasks</h4><p>除了上面两个减少模型参数的方法以外，论文中还设计了两个提高模型精度对方法（trick？）<br>第一个就是设计了一个新的预训练策略：<br><strong>SOP：</strong>Sentence order prediction，摒弃了原来bert采取的NSP预训练策略，这两种方法的主要不同在于：SOP的负样本是把正样本的顺序进行颠倒，而不是如NSP是从另外一个document中采样第二句来作为负样本。<br>具体如图：<br><strong>SOP</strong><br><img src="/Users/chennuo/Desktop/%E6%88%AA%E5%B1%8F2020-04-25%E4%B8%8A%E5%8D%8810.47.49.png" alt="image.png"><br><strong>NSP</strong><br><img src="/Users/chennuo/Desktop/%E6%88%AA%E5%B1%8F2020-04-25%E4%B8%8A%E5%8D%8810.47.53.png" alt="image.png"></p>
<ul>
<li><strong>实验结果</strong><br><img src="/Users/chennuo/Desktop/%E6%88%AA%E5%B1%8F2020-04-25%E4%B8%8A%E5%8D%8810.47.59.png" alt="image.png"><h4 id="Remove-dropout"><a href="#Remove-dropout" class="headerlink" title="Remove dropout"></a>Remove dropout</h4>第二个方法就是移除了dropout</li>
<li><strong>实验结果：</strong><br><img src="/Users/chennuo/Desktop/%E6%88%AA%E5%B1%8F2020-04-25%E4%B8%8A%E5%8D%8810.48.06.png" alt="image.png"><br>移除了dropout以后，作者们发现了两点：</li>
<li>第一就是模型的精度不降反升（这点很值得思考 why？？？？？）；</li>
<li>第二就是大大减少了模型在训练过程中对显存的使用量；<h2 id="三、Summery"><a href="#三、Summery" class="headerlink" title="三、Summery"></a>三、Summery</h2></li>
<li>论文一共提出了两种减少模型参数的方法和两种提高模型精度的策略，对于实验的分析十分清楚，建议去看原文，或者去看ALbert作者Dc.Lan曾经对该论文的直播讲述（<a href="https://www.bilibili.com/video/BV1C7411c7Ag?p=4）" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1C7411c7Ag?p=4）</a></li>
<li>关于dropout 移除以后，模型的精度却有所提升，这一点我并没找到合理的解释？？也欢迎大家留言😊；</li>
<li>就是albert其实并没有真正的提高模型的推理速度，貌似是因为矩阵因式分解很耗时间？？？或许是因为self-attention在整个transformer模型中所占用的时间很少？（1/10？）</li>
<li>模型未来可以与  model distillation与 sparse attention相结合来提高推理速度，这应该是一个不错的研究方向</li>
</ul>
]]></content>
      <tags>
        <tag>bert</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2020/04/24/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
  <entry>
    <title>justtry</title>
    <url>/2020/04/24/justtry/</url>
    <content><![CDATA[<h1 id="toc_0" align='center'>Assignment 8
</h1>
<h3 id="toc_0" align='center'>陈诺
</h3>
### 1.Edit Distance

<ul>
<li><p>解题思路：用动态规划的思路，维护一个二维的数组 dp，其大小为 mxn，m和n分别为 word1 和 word2 的长度。dp[i][j] 表示从 word1 的前i个字符转换到 word2 的前j个字符所需要的步骤。先给这个二维数组 dp 的第一行第一列赋值，这个很简单，因为第一行和第一列对应的总有一个字符串是空串，于是转换步骤完全是另一个字符串的长度。<br>状态转移方程：</p>
<pre><code>dp[i][j] =     /    dp[i - 1][j - 1]                                                                   if word1[i - 1] == word2[j - 1]

             \    min(dp[i - 1][j - 1], min(dp[i - 1][j], dp[i][j - 1])) + 1            else</code></pre></li>
</ul>
<ul>
<li><p>代码函数如下：</p>
<p><img src="/Users/chennuo/Desktop/%E6%88%AA%E5%B1%8F2020-04-23%E4%B8%8B%E5%8D%888.00.37.png" alt=""></p>
</li>
<li><p>运行结果：<br><img src="/Users/chennuo/Desktop/%E6%88%AA%E5%B1%8F2020-04-23%E4%B8%8B%E5%8D%888.00.32.png" alt=""></p>
</li>
</ul>
<h3 id="2-Burst-balloons"><a href="#2-Burst-balloons" class="headerlink" title="2.Burst balloons"></a>2.Burst balloons</h3><ul>
<li>解题思路：</li>
</ul>
<p><strong>分析最优子结构</strong></p>
<p><strong>正向思考</strong></p>
<p>以 [3, 1, 5][3,1,5] 为例，对于第一次选择，我们有三种选择。</p>
<p>选择 3 将问题分解为左边的子问题： [][] 和 右边子问题： [1, 5][1,5]</p>
<p>其解等于： [] + [1, 5] + 1 * 3 * 1[]+[1,5]+1∗3∗1</p>
<p>选择1 将问题分解为左边的子问题： [3][3] 和 右边子问题： [5][5]</p>
<p>其解等于： [3] + [5] + 3 * 1 * 5[3]+[5]+3∗1∗5</p>
<p>选择5 将问题分解为左边的子问题： [3, 1][3,1] 和 右边子问题： [][]</p>
<p>其解等于： [3, 1] + [] * 1 * 5 * 1[3,1]+[]∗1∗5∗1</p>
<p>发现，上面的解时错的。因为正向思考的情况下，以选择 1 为例，在点爆 1 气球后，两个左右子问题并不是独立的，所以这给计算子问题带来了障碍，怎么处理才能忽略中间的 1 呢？</p>
<p><strong>逆向思考</strong></p>
<p>以 [3, 1, 5][3,1,5] 为例，首先拿一个气球，把这个气球当做最后一个气球，然后点爆它。这样就能够将这个气球的左右两个子问题独立开。</p>
<p>换种方式来说，我们选这1这个气球，然后优先点爆左边和右边的气球之后，再最后点爆这个气嘴，这时可以看出左右两个子问题是独立的，他们只和1这个气球有关联。</p>
<p>定义状态转移方程<br>dp[i][j]dp[i][j] 从 ii 到 jj 个气球（闭区间）能够获取的最大的硬币数量</p>
<p>dp[i][j] = dp[i][k - 1] + dp[k + 1][j] + nums[i - 1] * nums[k] * nums[j + 1]<em>(i &lt;= k &lt;= j) dp[i][j]=dp[i][k−1]+dp[k+1][j]+nums[i−1]*nums[k] * nums *[j+1]</em>(i&lt;=k&lt;=j)</p>
<ul>
<li>函数代码与运行结果：<br><img src="/Users/chennuo/Desktop/%E6%88%AA%E5%B1%8F2020-04-22%E4%B8%8B%E5%8D%889.39.56.png" alt=""></li>
</ul>
<h3 id="3-My-Calendar"><a href="#3-My-Calendar" class="headerlink" title="3.My Calendar"></a>3.My Calendar</h3><ul>
<li><p>思路：我们用一个map来建立起始时间和结束时间的映射，map会按照起始时间进行自动排序。然后对于新进来的区间，我们在已有区间中查找第一个不小于新入区间的起始时间的区间，如果这个区间存在的话，说明新入区间的起始时间小于等于当前区间，也就是解法一中的第二个if情况，当前区间起始时间小于新入区间结束时间的话返回false。我们还要跟前面一个区间进行查重叠操作，那么判断如果当前区间不是第一个区间的话，就找到前一个区间，此时是解法一中第一个if情况，并且如果前一个区间的结束时间大于新入区间的起始时间的话，返回false。否则就建立新的映射，返回true即可</p>
</li>
<li><p>代码：<img src="/Users/chennuo/Desktop/%E6%88%AA%E5%B1%8F2020-04-23%E4%B8%8B%E5%8D%888.43.04.png" alt=""></p>
</li>
<li><p>运行结果：<img src="/Users/chennuo/Desktop/%E6%88%AA%E5%B1%8F2020-04-23%E4%B8%8B%E5%8D%888.42.59.png" alt=""></p>
</li>
</ul>
]]></content>
      <tags>
        <tag>leetcode</tag>
      </tags>
  </entry>
</search>
